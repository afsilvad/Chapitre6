---
title: "Chapitre 6"
author: "Andr√©s Silva"
date: "12/02/2020"
output: html_document
---

Une **variable** est l‚Äôobservation d‚Äôune caract√©ristique d√©crivant un √©chantillon.

En R, une variable est associ√©e √† un vecteur ou une colonne d‚Äôun tableau.

```{r}
rho <- c(1.34, 1.52, 1.26, 1.43, 1.39) # Cr√©ation de la variable rho
data <- data.frame(rho = rho) # tableau
data
```

_Variables quantitatives_ : Continues dans une espace √©chantillonal r√©el.
_Variables qualitatives_ : une couleur ou une s√©rie de sol. Une variable impossible √† mesurer num√©riquement.

L‚Äôinterpr√©tation __bay√©sienne__ vise √† quantifier l‚Äôincertitude des ph√©nom√®nes. Dans cette perspective, plus l‚Äôinformation s‚Äôaccumule, plus l‚Äôincertitude diminue. L‚Äôapproche bay√©sienne √©value la probabilit√© que le mod√®le soit r√©el.

Des rivalit√©s factices s‚Äôinstallent enter les tenants des diff√©rentes approches, dont chacune, en r√©alit√©, r√©pond √† des questions diff√©rentes dont il convient r√©fl√©chir sur les limitations.

***Les distributions***

Toujours, l‚Äôaire sous la courbe d‚Äôune distribution de probabilit√© est √©gale √† 1

En tant que sc√©nario √† deux issues possibles, des tirages √† pile ou face suivent une loi binomiale, comme toute variable bool√©enne prenant une valeur vraie ou fausse.

La distribution de Poisson n‚Äôa qu‚Äôun seul param√®tre $\lambda$, qui d√©crit tant la moyenne des d√©comptes.

```{r message=FALSE, warning=FALSE}
library("tidyverse")
x <- 0:25
y <- dbinom(x = x, size = 25, prob = 0.5)
print(paste('La somme des probabilit√©s est de', sum(y)))

ggplot(data = tibble(x, y), mapping = aes(x, y)) +
  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = "grey50") +
  geom_point()
```

La distribution la plus simple est probablement la distribution uniforme. On utilise rarement la distribution uniforme en biostatistiques, sinon pour d√©crire des a priori vagues pour l‚Äôanalyse bay√©sienne.

Les distributions de mesures exclusivement positives (comme le poids ou la taille) sont parfois avantageusement approxim√©es par une loi log-normale, qui est une loi normale sur le logarithme des valeurs: la moyenne d‚Äôune loi log-normale est la moyenne g√©om√©trique.

```{r message = FALSE, warning = FALSE}
increment <- 0.01
x <- seq(-10, 10, by = increment)
y1 <- dnorm(x, mean = 0, sd = 1)
y2 <- dnorm(x, mean = 0, sd = 2)
y3 <- dnorm(x, mean = 0, sd = 3)

print(paste('La somme des probabilit√©s est de', sum(y3 * increment)))

gg_norm <- data.frame(x, y1, y2, y3) %>% gather(variable, value, -x)

ggplot(data = gg_norm, mapping = aes(x = x, y = value)) +
  geom_line(aes(colour = variable))
```

Quelle est la probabilit√© d‚Äôobtenir le nombre 0 chez une observation continue distribu√©e normalement dont la moyenne est 0 et l‚Äô√©cart-type est de 1? R√©ponse: 0. La loi normale √©tant une distribution continue, les probabilit√©s non-nulles ne peuvent √™tre calcul√©s que sur des intervalles. Par exemple, la probabilit√© de retrouver une valeur dans l‚Äôintervalle entre -1 et 2 est calcul√©e en soustrayant la probabilit√© cumul√©e √† -1 de la probabilit√© cumul√©e √† 2.

```{r message = FALSE, warning = FALSE}
increment <- 0.01
x <- seq(-5, 5, by = increment)
y <- dnorm(x, mean = 0, sd = 1)

prob_between <- c(-1, 2)

gg_norm <- data.frame(x, y)
gg_auc <- gg_norm %>%
  filter(x > prob_between[1], x < prob_between[2]) %>%
  #rbind() utilis√© pour bien fermer le polygon dans ggplot
  rbind(c(prob_between[2], 0)) %>%
  rbind(c(prob_between[1], 0))

ggplot(data.frame(x, y), aes(x, y)) +
  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexad√©cimal
  geom_line()
```

```{r message = FALSE, warning = FALSE}
# La fonction pnorm calcule la probabilit√© d'une valeur 
prob_norm_between <- pnorm(q = prob_between[2], mean = 0, sd = 1) - pnorm(q = prob_between[1], mean = 0, sd = 1)
print(paste("La probabilit√© d'obtenir un nombre entre", 
            prob_between[1], "et", 
            prob_between[2], "est d'environ", 
            round(prob_norm_between, 2) * 100, "%"))
```

La figure ci-dessous montre l'aire sous la courbe r√©presentant 95% de la population

```{r message = FALSE, warning = FALSE}
increment <- 0.01
x <- seq(-5, 5, by = increment)
y <- dnorm(x, mean = 0, sd = 1)

alpha <- 0.05
prob_between <- c(qnorm(p = alpha/2, mean = 0, sd = 1),
                  qnorm(p = 1 - alpha/2, mean = 0, sd = 1))

gg_norm <- data.frame(x, y)
gg_auc <- gg_norm %>%
  filter(x > prob_between[1], x < prob_between[2]) %>%
  rbind(c(prob_between[2], 0)) %>%
  rbind(c(prob_between[1], 0))

ggplot(data = data.frame(x, y), mapping = aes(x, y)) +
  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexad√©cimal
  geom_line() +
  geom_text(data = data.frame(x = prob_between,
                              y = c(0, 0),
                              labels = round(prob_between, 2)),
            mapping = aes(label = labels))
```

En effet, la moyenne suit aussi une distribution normale, dont la tendance centrale est la moyenne de la distribution, et dont **l‚Äô√©cart-type est not√© erreur standard**.On calcule cette erreur en divisant **la variance par le nombre d‚Äôobservations, ou en divisant l‚Äô√©cart-type par la racine carr√©e du nombre d‚Äôobservations**.

```{r}
increment <- 0.01
x <- seq(-5, 5, by = increment)
y <- dnorm(x, mean = 0, sd = 1)

alpha <- 0.05
prob_between <- c(qnorm(p = alpha/2, mean = 0, sd = 1) / sqrt(10),
                  qnorm(p = 1 - alpha/2, mean = 0, sd = 1) / sqrt(10))

gg_norm <- data.frame(x, y)
gg_auc <- gg_norm %>%
  filter(x > prob_between[1], x < prob_between[2]) %>%
  rbind(c(prob_between[2], 0)) %>%
  rbind(c(prob_between[1], 0))

ggplot(data = data.frame(x, y), mapping = aes(x, y)) +
  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexad√©cimal
  geom_line() +
  geom_text(data = data.frame(x = prob_between,
                              y = c(0, 0),
                              labels = round(prob_between, 2)),
            mapping = aes(label = labels))
```

**Statistiques descriptives**

```{r message = FALSE, warning = FALSE}
data("iris")

#R√©sum√© des statistiques des variables contenues dans le data.frame iris
summary(iris)

# Moyenne des diff√©rentes variables s√©lon le type d'esp√®ce
iris %>%
  group_by(Species) %>%
  summarise_all(mean)

# Quartiles 25, 50 et 75 des diff√©erentes variables s√©lon l'esp√®ce
iris %>%
  group_by(Species) %>%
  summarise_all(list(q25 = ~ quantile(., probs = 0.25), 
                     q50 = ~ quantile(., probs = 0.50),
                     q75 = ~ quantile(., probs = 0.75)))

# Fonction quantile() pour calculer les quartiles 
quantile(iris$Sepal.Length[iris$Species == 'setosa'])

# table() d√©compte par cat√©gorie. prop.table() obtient les proportions
tableau_croise <- table(iris$Species, 
                        cut(iris$Sepal.Length, breaks = quantile(iris$Sepal.Length)))

prop.table(tableau_croise)
```

**Test d'hypoth√®ses**

Par convention, l‚Äôhypoth√®se nulle (√©crite $H_{0}$) est l‚Äôhypoth√®se qu‚Äôil n‚Äôy ait pas d‚Äôeffet (c‚Äôest l‚Äôhypoth√®se de l‚Äôavocat du diable üòà) √† l‚Äô√©chelle de la population (et non pas √† l‚Äô√©chelle de l‚Äô√©chantillon). √Ä l‚Äôinverse, l‚Äôhypoth√®se alternative (√©crite $H_{1}$) est l‚Äôhypoth√®se qu‚Äôil y ait un effet √† l‚Äô√©chelle de la population.

_Test de t √† un seul √©chantillon_

L'erreur standard sur la moyenne $ESM = \frac{s}{\sqrt{n}}$

```{r}
set.seed(33746)

# cr√©ation d'une population avec distribution normale n = 20, moyenne = 16 et sd = 4
x <- rnorm(20, 16, 4)

level <-  0.95
alpha <- 1-level

x_bar <- mean(x)
s <- sd(x)
n <- length(x)

error <- qnorm(1 - alpha/2) * s / sqrt(n)
error

# Intervalle de confiance
c(x_bar - error, x_bar + error)
```

Si la moyenne de la population est de 16, un nombre qui se situe dans l‚Äôintervalle de confiance on accepte l‚Äôhypoth√®se nulle au seuil 0.05. Si le nombre d‚Äô√©chantillon est r√©duit (g√©n√©ralement < 30), on passera plut√¥t par une **distribution de t**, avec $n-1$ degr√©s de libert√©.

```{r}
error <- qt(1 - alpha/2, n-1) * s / sqrt(n)
c(x_bar - error, x_bar + error)

#test de t pour v√©rifier si les donn√©es proviennent d'une population dont la moyenne est 18
t.test(x, mu = 18)
```

En apliquant le _test de t_ on constate que l'hypoth√®se altyernative est vraie car $16 \neq 18$.

**Test de Wicolson**

Contrairement au test t, le test de Wicolson ne demand pas des suppositions sur la distribution, car ce test est non-param√©trique bas√© sur le tri des valeurs. 

```{r message = FALSE, warning = FALSE}
# La valeur V correspond la somme des rangs positifs

wilcox.test(x, mu = 18)
```

_Test de t √† deux √©chantillons_

Deux s√©ries de donn√©es $x_{1}$ et $x_{2}$ issus de distribution normale, donc, on teste l'hypoth√®se nulle $\mu_{1}$ et $\mu_{2}$. Ainsi, t est calcul√©e comme :

$$t = \frac{\tilde{x_{1}}-\tilde{x_{1}}}{ESDM}$$
L‚ÄôESDM est l‚Äôerreur standard de la diff√©rence des moyennes

$$ESDM = \sqrt{ESM_{1}^{2} + ESM_{2}^{2}}$$

Exemple avec iris et la longueur des p√©tales

```{r message = FALSE, warning = FALSE}
iris_pl <- iris %>% 
    filter(Species != "setosa") %>%
    select(Species, Petal.Length)
sample_n(iris_pl, 5)

# La variable de sortie est Petal.Length qui varie en foction du groupe Species (Variable d'entr√©e)
t.test(formula = Petal.Length ~ Species,
       data = iris_pl, var.equal = FALSE)
tt_pl <- t.test(formula = Petal.Length ~ Species,
                data = iris_pl, var.equal = FALSE)
summary(tt_pl)
str(tt_pl)

# Comparaison des variances avec le test de F (Fisher)
var.test(formula = Petal.Length ~ Species,
         data = iris_pl)

# Test de Wilcoxon √† deux √©chantillons, m√™me formule que pour test t

wilcox.test(formula = Petal.Length ~ Species,
       data = iris_pl, var.equal = TRUE)
```

_Les test pair√©s_

Utilis√©s lorsque deux √©chantollons proviennent d'une m√™me unit√© exp√©rimentale, il s‚Äôagit en fait de tests sur la diff√©rence entre deux observations

```{r message = FALSE, warning = FALSE}

set.seed(2555)

n <- 20
avant <- rnorm(n, 16, 4)
apres <- rnorm(n, 18, 3)

# Par default paired est FALSE, il faut le mettre comme TRUE pour fair le test pair√©
t.test(avant, apres, paired = TRUE)

wilcox.test(avant, apres, paired = TRUE)
```

**L'analyse de variance**

L‚Äôanalyse de variance consiste √† comparer des moyennes de plusieurs groupe distribu√©s normalement et de m√™me variance.

```{r}
pl_aov <- aov(Petal.Length ~ Species, iris)
summary(pl_aov)
```

__Les mod√®les statistiques__

La mod√©lisation statistique consiste √† lier de mani√®re explicite des variables de sortie $y$ (ou variables-r√©ponse ou variables d√©pendantes) √† des variables explicatives $x$ (ou variables pr√©dictives / ind√©pendantes / covariables).

_Mod√®les pr√©dictifs_ con√ßues pour pr√©dire de mani√®re fiable une ou plusieurs variables-r√©ponse √† partir des informations contenues dans les variables qui sont, dans ce cas, pr√©dictives.

Lorsque l‚Äôon d√©sire tester des hypoth√®ses pour √©valuer quelles variables expliquent la r√©ponse, on parlera de _mod√©lisation (et de variables) explicatives_.

Les _Variables fixes_ sont les variables test√©es lors de l'exp√©rience : dose du traitement, esp√®ce, m√©t√©o.

Les _Variables al√©atoires_ sont les sources de variation qui g√©n√®rent du bruit dans le mod√®le : les unit√©s exp√©rimentales ou le temps lors de mesures r√©p√©t√©es.

_Mod√®les fixes_ juste effets fixes

_Mod√®les mixtes_ ont des variables al√©atoires et fixes.

- _Mod√®les √† effets fixes_

En particulier, le test de t est r√©gression lin√©aire univari√©e (√† une seule variable explicative) dont la variable explicative comprend deux cat√©gories. De m√™me, l‚Äôanova est une r√©gression lin√©aire univari√©e dont la variable explicative comprend plusieurs cat√©gories.

Mod√®le lineaire univari√© aura la forme $y = \beta_{0} + \beta_{1}x + \epsilon$, o√π $\epsilon$ est l'erreur.

- _Mod√®le lin√©aire univari√© avec variable continue_

```{r message = FALSE, warning = FALSE}
library("agridat")
data("lasrosas.corn")
sample_n(lasrosas.corn, 10)

# nitro comme variable explicative ou ind√©pendante et yield comme variable d√©pendante
ggplot(data = lasrosas.corn, mapping = aes(x = nitro, y = yield)) +
    geom_point()

modlin_1 <- lm(yield ~ nitro, data = lasrosas.corn)
summary(modlin_1)
```

Des r√©sultats obtenus la m√©diane des r√©sidus devrait s'approcher de la moyenne des r√©sidus (toujours 0). Bien que le -3.079 peut sembler important, il faut prendre en consid√©ration de l‚Äô√©chelle de y. La distribution des r√©sidus m√©rite d‚Äô√™tre davantage investigu√©e.

```{r message = FALSE, warning = FALSE}
# Calcul des intervalles de confiance
confint(modlin_1, level = 0.95)

# Dans le r√©sultat de la fonction coefficients() on obtient l'intercept et la pente
coefficients(modlin_1)

# √âxecution du mod√®le sur les donn√©es qui ont servi √† le g√©n√©rer
predict(modlin_1)[1:5]

#Ou sur des donn√©es externes
nouvelles_donnees <- data.frame(nitro = seq(from = 0, to = 100, by = 5))
predict(modlin_1, newdata = nouvelles_donnees)[1:5]

```
- _Analyse des r√©sidus_

Vecteur $\epsilon$ qui est un d√©calage entre le donn√©es et le mod√®le. Les r√©sidus sont le r√©sultat de $\epsilon = y - \hat{y}$ ou d'utiliser la fonction `residuals()`.

```{r message = FALSE, warning = FALSE}
res_df <- data.frame(nitro = lasrosas.corn$nitro,
                     residus_lm = residuals(modlin_1), 
                     residus_calcul = lasrosas.corn$yield - predict(modlin_1))
sample_n(res_df, 10)
```

Dans une bonne r√©gression lin√©aire, on ne retrouvera pas de structure identifiable dans les r√©sidus, c‚Äôest-√†-dire que les r√©sidus sont bien distribu√©s de part et d‚Äôautre du mod√®le de r√©gression.

```{r message = FALSE, warning = FALSE}
ggplot(res_df, aes(x = nitro, y = residus_lm)) +
  geom_point() +
  labs(x = "Dose N", y = "R√©sidus") +
  geom_hline(yintercept = 0, col = "red", size = 1)
```

On pourra aussi inspecter les r√©sidus avec un graphique de leur distribution. L‚Äôhistogramme devrait pr√©senter une distribution normale. Les tests de normalit√© comme le test de Shapiro-Wilk peuvent aider, mais ils sont g√©n√©ralement tr√®s s√©v√®res.

```{r message = FALSE, warning = FALSE}
ggplot(res_df, aes(x = residus_lm)) +
  geom_histogram(binwidth = 2, color = "white") +
  labs(x = "Residual")

#Test de Shapito-Wilk
shapiro.test(res_df$residus_lm)
```

L‚Äôhypoth√®se nulle que la distribution est normale est rejet√©e au seuil 0.05. Dans notre cas, il est √©vident que la s√©v√©rit√© du test n‚Äôest pas en cause, car les r√©sidus semble g√©n√©rer trois ensembles. **Ceci indique que les variables explicatives sont insuffisantes pour expliquer la variabilit√© de la variable-r√©ponse**.

## **R√©gression multiple**

Lorsque l‚Äôon combine plusieurs variables explicatives, on cr√©e un mod√®le de r√©gression multivari√©e ou multiple :

$$y = X\beta + \epsilon$$

O√π, $X$ est la matrice du mod√®le √† $n$ observations et $p$ variables, $\beta$ est la matrice des $p$ coefficients, $\beta_{0}$ est l'intercept qui multiplie la premi√®re colonne de la matrice $X$ et $\epsilon$ est l'erreur de chaque observation.

_Mod√®les lin√©aires univari√©s avec variable cat√©gorielle nominale_

Une variable cat√©gorielle nominale (non ordonn√©e) utilis√©e √† elle seule dans un mod√®le comme variable explicative, est un cas particulier de r√©gression multiple.

```{r message = FALSE, warning = FALSE}
data <- data.frame(cultivar = c('Superior', 'Superior', 'Superior', 'Russet', 'Kenebec', 'Russet'))
model.matrix(~cultivar, data)
```

L‚Äôinformation contenue dans un nombre $C$ de cat√©gorie peut √™tre encod√©e dans un nombre $C - 1$ de colonnes. C‚Äôest pourquoi, dans une analyse statistique, on d√©signera une cat√©gorie comme une r√©f√©rence, que l‚Äôon d√©tecte lorsque toutes les autres cat√©gories sont encod√©es avec des $0$ : cette r√©f√©rence sera incluse dans _l‚Äôintercept_. Dans l'exemple ci-dessus l'intercept a √©t√© le cultivar _kenebec_. On peut modifier la r√©f√©rence avec la fonction `relevel()`.

```{r message = FALSE, warning = FALSE}
levels(lasrosas.corn$topo)
ggplot(lasrosas.corn, aes(x = topo, y = yield)) +
    geom_boxplot()

# La fonction model.matrix() sert √† g√©n√©rer l'encodage cat√©goriel
# Dans ce cas l'intercept est le niveau E
model.matrix(~ topo, data = lasrosas.corn) %>% 
    tbl_df() %>% # tbl_df pour transformer la matrice en tableau
    sample_n(10) 
```

Cette matrice de mod√®le utilis√©e pour la r√©gression donnera un intercept, qui indiquera l‚Äôeffet de la cat√©gorie de r√©f√©rence, puis les diff√©rences entre les cat√©gories subs√©quentes et la cat√©gorie de r√©f√©rence.

```{r message = FALSE, warning = FALSE}
modlin_4 <- lm(yield ~ topo, data = lasrosas.corn)
summary(modlin_4)

res_df2 <- data.frame(topo = lasrosas.corn$topo,
                     residus_lm = residuals(modlin_4), 
                     residus_calcul = lasrosas.corn$yield - predict(modlin_4))
sample_n(res_df2, 10)

ggplot(res_df2, aes(x = topo, y = residus_lm)) +
  geom_point() +
  labs(x = "Niveau Topo", y = "R√©sidus") +
  geom_hline(yintercept = 0, col = "red", size = 1)

#Histograme des r√©sidus r√©gression variable cat√©gorielle nominale
ggplot(res_df2, aes(x = residus_lm)) +
  geom_histogram(binwidth = 2, color = "white") +
  labs(x = "Residual")

#Test de Shapito-Wilk
shapiro.test(res_df2$residus_lm)
```

### **R√©gresion multiple √† plusieurs variables**

```{r message = FALSE, warning = FALSE}
head(lasrosas.corn)

# lat = latitude long = longitude bv = teneur en mati√®re organique plus bv = moins MO
modlin_5 <- lm(yield ~ lat + long + nitro + topo + bv,
               data = lasrosas.corn)
summary(modlin_5)
```

Les variables ne sont pas comparables puisque n'ont pas la m√™me √©chelle

```{r message = FALSE, warning = FALSE}
lasrosas.corn_sc <- lasrosas.corn %>%
    mutate_at(c("lat", "long", "nitro", "bv"), scale)

modlin_5_sc <- lm(yield ~ lat + long + nitro + topo + bv,
               data = lasrosas.corn_sc)
summary(modlin_5_sc)
```

On pourra retrouver des cas o√π l‚Äôeffet combin√© de plusieurs variables diff√®re de l‚Äôeffet des deux variables prises s√©par√©ment 

```{r message = FALSE, warning = FALSE}

# Dans nitro*topo l'√©toile (ast√©risque) sert √† √©valuer l'effet de l'interaction
modlin_5_sc <- lm(yield ~ nitro*topo,
               data = lasrosas.corn_sc)
summary(modlin_5_sc)
```

Les r√©sultats montre des effets de l‚Äôazote et des cat√©gories topographiques, mais il y a davantage d‚Äôincertitude sur les interactions, indiquant que l‚Äôeffet statistique de l‚Äôazote est sensiblement le m√™me ind√©pendamment des niveaux topographiques. 

**Il faut ne pas surcharger le mod√®le** puisque plus il y a d‚Äôinteractions, plus votre mod√®le comprendra de variables et vos tests d‚Äôhypoth√®se perdront en puissance statistique.